{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e451333d-9a36-4333-8454-00f9d00b2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import math \n",
    "import gmpy as g\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tf\n",
    "import torchvision.models as models\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import skimage.filters as sk_filters\n",
    "from scipy.spatial import distance_matrix, minkowski_distance, distance\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "from data_load_utility import *\n",
    "from module_train_utility import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf97d6c-0378-4c19-977b-c11af2f3b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3856dae-50ab-4873-88ce-4987857618ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1a2b01-7578-4112-b1cc-6dce88e6b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSI_load(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 train=True,\n",
    "                 fold=0, #index of test data\n",
    "                 crops= 128,\n",
    "                 adj=True,\n",
    "                 prune='Grid',\n",
    "                 neighs=8):\n",
    "        super(WSI_load,self).__init__()\n",
    "\n",
    "        self.img_dir = 'scaled image/'\n",
    "        self.meta_dir = 'anno_meta/'\n",
    "\n",
    "        names=[file for file in os.listdir(self.meta_dir) if not file.endswith('.DS_Store')]\n",
    "\n",
    "        self.train=train\n",
    "        self.adj = adj\n",
    "\n",
    "        samples=names\n",
    "        te_names=te_names\n",
    "        tr_names=list(set(samples)-set(te_names))\n",
    "\n",
    "        if train:\n",
    "            self.names = tr_names\n",
    "        else:\n",
    "            self.names = te_names\n",
    "        \n",
    "        print('Loading imgs...')\n",
    "        self.img_dict= {i:get_img(self.img_dir,i) for i in self.names}\n",
    "        \n",
    "        \n",
    "        print('Load imgs meta...')\n",
    "        self.meta_dict={\n",
    "            key:get_meta(value,crops) \n",
    "            for key, value in self.img_dict.items()\n",
    "        }\n",
    "\n",
    "        print('Tiling imgs...')\n",
    "        self.patch_dict={}\n",
    "        for key, value in self.img_dict.items(): #change to exclude norm\n",
    "            meta=self.meta_dict[key]\n",
    "            img_val=get_patch_noden(value,meta,crops)\n",
    "            self.patch_dict[key]=img_val\n",
    "            \n",
    "        print('Load filtered meta...')\n",
    "        self.meta_filter_dict={i:read_meta(self.meta_dir,i) for i in self.names}\n",
    "        \n",
    "        self.lbl_dict={key:torch.tensor(value['TLS_score'].values-1) for key, value in self.meta_filter_dict.items()}\n",
    "\n",
    "        print('Numpy array img to torch...')\n",
    "        self.patch_tensor_dict={}\n",
    "        for key,value in self.patch_dict.items():\n",
    "            meta=self.meta_dict[key]\n",
    "            meta_filter=self.meta_filter_dict[key]\n",
    "            patch_tensor=np_to_tensor(meta,meta_filter,value)\n",
    "            self.patch_tensor_dict[key]=patch_tensor\n",
    "\n",
    "        print('Get img coord and centers...')\n",
    "        #patch coordinates\n",
    "        self.coord_dict = {}\n",
    "        for key, value in self.meta_filter_dict.items():\n",
    "            coord = value[['coord_x','coord_y']].values.astype(np.float32)\n",
    "            #coord_tensor = torch.from_numpy(coord)\n",
    "            self.coord_dict[key]=coord\n",
    "\n",
    "        #pixel/image centers\n",
    "        self.centers_dict={}\n",
    "        for key, value in self.meta_filter_dict.items():\n",
    "            centers = value[['image_x','image_y']].values.astype(np.float32)\n",
    "            self.centers_dict[key]=centers\n",
    "\n",
    "        print('Calculate img adjacent patches use X/Y coordinates...')\n",
    "        self.adj_dict = {i:calcADJ(m,neighs,pruneTag=prune) for i,m in self.coord_dict.items()}\n",
    "\n",
    "        ## filter with adj\n",
    "        ### label of patches without neighbors\n",
    "        self.adj_f_dict = {}\n",
    "        self.patches_f_dict = {}\n",
    "        self.coord_f_dict = {}\n",
    "        self.centers_f_dict = {}\n",
    "        self.lbl_f_dict = {}\n",
    "        \n",
    "        for key,value in self.meta_filter_dict.items():\n",
    "            adj = self.adj_dict[key]\n",
    "            num_neigh = adj.sum(1,keepdim = True)\n",
    "            label = num_neigh.nonzero()\n",
    "            label = label[:,0]\n",
    "\n",
    "            ### for adj\n",
    "            adj_f = adj[:,label]\n",
    "            adj_f = adj_f[label,:]\n",
    "            self.adj_f_dict[key]=adj_f\n",
    "\n",
    "            ### for patches\n",
    "            patches = self.patch_tensor_dict[key]\n",
    "            patches_f = patches[label,:,:,:]\n",
    "            self.patches_f_dict[key]=patches_f\n",
    "\n",
    "            ###for coord\n",
    "            coord = self.coord_dict[key]\n",
    "            coord_f = coord[label,:]\n",
    "            self.coord_f_dict[key]=coord_f\n",
    "        \n",
    "            ### for centers\n",
    "            centers = self.centers_dict[key]\n",
    "            centers_f = centers[label,]\n",
    "            self.centers_f_dict[key]=centers_f\n",
    "\n",
    "            ### for lbls\n",
    "            lbls = self.lbl_dict[key]\n",
    "            lbls_f = lbls[label]\n",
    "            self.lbl_f_dict[key]=lbls_f\n",
    "\n",
    "        self.id2name = dict(enumerate(self.names))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        ID=self.id2name[index]\n",
    "        patches_f=self.patches_f_dict[ID]\n",
    "        coord_f=self.coord_f_dict[ID]\n",
    "        positions = torch.LongTensor(coord_f)\n",
    "        centers_f=self.centers_f_dict[ID]\n",
    "        centers_ft = torch.LongTensor(centers_f)\n",
    "        adj_f=self.adj_f_dict[ID]\n",
    "        label_f=self.lbl_f_dict[ID]\n",
    "        \n",
    "        data = [patches_f,positions,centers_ft,label_f]\n",
    "        if self.adj:\n",
    "            data.append(adj_f)\n",
    "\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        return len(self.centers_f_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886a94d-4acc-49aa-a0d9-51cda2bba521",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('your_out_dir/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83687226-8cd3-4ae5-82df-9cb48ba687fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09876709', '11417008', '11573400', '11612494', '11634225', '11654963', '11730621', '11769083']\n",
      "Loading imgs...\n",
      "Load imgs meta...\n",
      "Tiling imgs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filtered meta...\n",
      "Numpy array img to torch...\n",
      "Get img coord and centers...\n",
      "Calculate img adjacent patches use X/Y coordinates...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.WSI_load at 0x15534b22b010>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset=WSI_load(train=True,fold = 1, adj=True,crops=128,neighs=8,prune='Grid')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db260e44-c622-4839-b131-0cdf20191ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09876709', '11417008', '11573400', '11612494', '11634225', '11654963', '11730621', '11769083']\n",
      "Loading imgs...\n",
      "Load imgs meta...\n",
      "Tiling imgs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n",
      "Tiling image: 100%|██████████ [ time left: 00:00 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filtered meta...\n",
      "Numpy array img to torch...\n",
      "Get img coord and centers...\n",
      "Calculate img adjacent patches use X/Y coordinates...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.WSI_load at 0x15534b22ba00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset=WSI_load(train=False,fold = 1, adj=True,crops=128,neighs=8,prune='Grid')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342660be-896b-43ac-9c86-cac72a8c095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train' : train_dataset,\n",
    "           'val' : test_dataset}\n",
    "dataloaders = {x:DataLoader(datasets[x],batch_size=1,num_workers=2,shuffle=True) for x in ['train','val']}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12fc2aad-4780-4291-9868-5eb9d487d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creading Models\n",
    "'''\n",
    "#device\n",
    "device=(\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    #else 'mps'\n",
    "    #if torch.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(f'using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2ff22e-72be-4427-8974-0af294719057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "class PatchClassifier(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 fig_size=128,\n",
    "                 dropout=0.2,\n",
    "                 n_pos = 135, #maxium of all meta (need adjust)\n",
    "                 kernel_size=4, #for convmixer block Conv2d\n",
    "                 patch_size=8, #for patch embedding adjust [n,32,16,16]\n",
    "                 num_class=4,\n",
    "                 depth1=2, #convmixer block layer, for adjust\n",
    "                 depth2=8, #attention block layer, for adjust\n",
    "                 depth3=4, #gnn block layer, for adjust\n",
    "                 heads=16, #number of heads in multihead attention in attn block for adjust,\n",
    "                 channel=32,\n",
    "                 policy='mean'):\n",
    "        super().__init__()\n",
    "\n",
    "        #self parameters\n",
    "        dim=(fig_size//patch_size)**2*channel//8\n",
    "        dim_head=dim/heads\n",
    "    \n",
    "        #self functions\n",
    "        ## embedding multi-dimension data to 2D, for ConvDepwise and Convpointwise\n",
    "        self.patch_embedding=torch.nn.Conv2d(3,channel,patch_size,patch_size)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        ## convmixer moduel\n",
    "        self.layer1=nn.Sequential(\n",
    "            *[convmixer_block(channel,kernel_size) for i in range(depth1)],\n",
    "        )\n",
    "\n",
    "        ## MLP\n",
    "        self.down=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel//8,1,1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        ## Add centers\n",
    "        self.x_embed=nn.Embedding(n_pos,dim)\n",
    "        self.y_embed=nn.Embedding(n_pos,dim)\n",
    "\n",
    "\n",
    "        ## Transformer module\n",
    "        self.layer2 = nn.Sequential(*[attn_block(dim,heads,dim_head,dim,dropout) for i in range(depth2)])\n",
    "        \n",
    "        ## GNN module\n",
    "        self.layer3 = nn.ModuleList([gs_block(dim,dim,policy,True) for i in range(depth3)])\n",
    "\n",
    "\n",
    "        ## LSTM\n",
    "        self.jknet = nn.Sequential(\n",
    "            nn.LSTM(dim,dim,2), #2 means the layer of LSTM, for adjusted\n",
    "            SelectItem(0), #output of LSTM contains 2 elements, (0) is ct, (1) is hidden layer\n",
    "        )\n",
    "\n",
    "        ## MLP to class\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim,num_class),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,patches,positions,adj):\n",
    "        B,N,C,H,W=patches.shape #B, N, 3, 128, 128\n",
    "        patches=patches.reshape(B*N,C,H,W)\n",
    "        \n",
    "        #patch embedding:extracting morphology info from each patch\n",
    "        patch_emb=self.patch_embedding(patches)  #from [n,3,128,128] to [n,32,16,16]\n",
    "\n",
    "        #convmixer module\n",
    "        x=self.dropout(patch_emb)\n",
    "        x=self.layer1(x)\n",
    "\n",
    "        #MLP\n",
    "        x = self.down(x)\n",
    "        g = x.unsqueeze(0)\n",
    "\n",
    "        # add centers\n",
    "        centers_x = self.x_embed(positions[:,:,0])\n",
    "        centers_y = self.y_embed(positions[:,:,1])\n",
    "        ct = centers_x+centers_y\n",
    "\n",
    "        #transformer module\n",
    "        layer2_input = g+ct\n",
    "        g=self.layer2(layer2_input).squeeze(0)\n",
    "\n",
    "        #gnn module\n",
    "        jk = []\n",
    "        for layer in self.layer3:\n",
    "            g = layer(g,adj)\n",
    "            jk.append(g.unsqueeze(0))\n",
    "        \n",
    "        g = torch.cat(jk,0)\n",
    "\n",
    "        #LSTM\n",
    "        g = self.jknet(g).mean(0)\n",
    "\n",
    "        #MLP to num_class\n",
    "        x = self.class_head(g)\n",
    "        \n",
    "        \n",
    "        #prob of each class\n",
    "        #prob = F.softmax(pred,dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd708e1a-25a1-4adc-9211-1c57f4bc7ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchClassifier(\n",
      "  (patch_embedding): Conv2d(3, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): convmixer_block(\n",
      "      (dw): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=32)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=32)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): GELU(approximate='none')\n",
      "      )\n",
      "      (pw): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): convmixer_block(\n",
      "      (dw): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=32)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), padding=same, groups=32)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): GELU(approximate='none')\n",
      "      )\n",
      "      (pw): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down): Sequential(\n",
      "    (0): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (x_embed): Embedding(200, 1024)\n",
      "  (y_embed): Embedding(200, 1024)\n",
      "  (layer2): Sequential(\n",
      "    (0): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): attn_block(\n",
      "      (attn): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): Attention(\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ff): PreNorm(\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fn): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.2, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (4): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): ModuleList(\n",
      "    (0-3): 4 x gs_block()\n",
      "  )\n",
      "  (jknet): Sequential(\n",
      "    (0): LSTM(1024, 1024, num_layers=2)\n",
      "    (1): SelectItem()\n",
      "  )\n",
      "  (class_head): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PatchClassifier(fig_size=128,\n",
    "                       dropout=0.2,\n",
    "                       n_pos=200,\n",
    "                       kernel_size=4,\n",
    "                       patch_size=8,\n",
    "                       num_class=4,\n",
    "                       depth1=2,\n",
    "                       depth2=8,\n",
    "                       depth3=4,\n",
    "                       heads=16,\n",
    "                       channel=32,\n",
    "                       policy='mean').to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb95c708-c429-4b8d-b2e0-91bc23dca9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(torch.nn.Module):\n",
    "    def __init__(self,model,weight_decay,p=2):\n",
    "        '''\n",
    "        :param model \n",
    "        :param weight_decay:\n",
    "        :param p: 2 default, 0 L2, 1 L1.\n",
    "        '''\n",
    "        super(Regularization, self).__init__()\n",
    "        if weight_decay <= 0:\n",
    "            print(\"param weight_decay can not <=0\")\n",
    "            exit(0)\n",
    "        self.model=model\n",
    "        self.weight_decay=weight_decay\n",
    "        self.p=p\n",
    "        self.weight_list=self.get_weight(model)\n",
    "        self.weight_info(self.weight_list)\n",
    " \n",
    "    def to(self,device):\n",
    "        '''\n",
    "        :param device: cude or cpu\n",
    "        :return:\n",
    "        '''\n",
    "        self.device=device\n",
    "        super().to(device)\n",
    "        return self\n",
    " \n",
    "    def forward(self, model):\n",
    "        self.weight_list=self.get_weight(model)#get the newest weight\n",
    "        reg_loss = self.regularization_loss(self.weight_list, self.weight_decay, p=self.p)\n",
    "        return reg_loss\n",
    " \n",
    "    def get_weight(self,model):\n",
    "        '''\n",
    "        get the newest weight list\n",
    "        :param model:\n",
    "        :return:\n",
    "        '''\n",
    "        weight_list = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                weight = (name, param)\n",
    "                weight_list.append(weight)\n",
    "        return weight_list\n",
    " \n",
    "    def regularization_loss(self,weight_list, weight_decay, p=2):\n",
    "        '''\n",
    "        :param weight_list:\n",
    "        :param p: \n",
    "        :param weight_decay:\n",
    "        :return:\n",
    "        '''\n",
    "        # weight_decay=Variable(torch.FloatTensor([weight_decay]).to(self.device),requires_grad=True)\n",
    "        # reg_loss=Variable(torch.FloatTensor([0.]).to(self.device),requires_grad=True)\n",
    "        # weight_decay=torch.FloatTensor([weight_decay]).to(self.device)\n",
    "        # reg_loss=torch.FloatTensor([0.]).to(self.device)\n",
    "        reg_loss=0\n",
    "        for name, w in weight_list:\n",
    "            l2_reg = torch.norm(w, p=p)\n",
    "            reg_loss = reg_loss + l2_reg\n",
    " \n",
    "        reg_loss=weight_decay*reg_loss\n",
    "        return reg_loss\n",
    " \n",
    "    def weight_info(self,weight_list):\n",
    "        '''\n",
    "        :param weight_list:\n",
    "        :return:\n",
    "        '''\n",
    "        print(\"---------------regularization weight---------------\")\n",
    "        for name ,w in weight_list:\n",
    "            print(name)\n",
    "        print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a2d53-b3bd-4137-9e25-9e00d3230a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "optimizing the model parameters\n",
    "'''\n",
    "\n",
    "#class weight\n",
    "tr_labs=[]\n",
    "for key, value in train_dataset.lbl_dict.items():\n",
    "    tr_labs.append(value)\n",
    "class_weight = compute_class_weight('balanced',\n",
    "                                    classes=[0,1,2,3],\n",
    "                                    y=torch.concat(tr_labs).numpy())\n",
    "\n",
    "class_weight = torch.Tensor(class_weight)\n",
    "class_weight = class_weight.to(device)\n",
    "\n",
    "\n",
    "#weight_decay\n",
    "weight_decay=10.0 \n",
    "reg_loss=Regularization(model, weight_decay, p=2).to(device)\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "StepLR=torch.optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eccd0d8-0a65-48fb-bec5-e6cdc980b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(filename,verbosity=1,name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter=logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger=logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh=logging.FileHandler(filename,'w')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh=logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd658f0-563a-449c-b187-ff036675bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,loss_fn,optimizer,StepLR,num_epochs=350):\n",
    "    \n",
    "    since=time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss=0.0\n",
    "            running_corrects=0\n",
    "\n",
    "            for batch, (patches,position,_,label,adj) in enumerate(dataloaders[phase]):\n",
    "                patches,position,adj,label, = patches.to(device),position.to(device),adj.to(device),label.to(device)\n",
    "                label = label.squeeze(0)\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    preds=model(patches,position,adj)\n",
    "                    _,probs=torch.max(preds,1)\n",
    "                    loss = loss_fn(preds, label)\n",
    "                    loss = loss + 1e-2 * reg_loss(model)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "                running_loss += loss.item() * patches.size(0)\n",
    "                accuracy = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
    "                running_corrects += accuracy(probs,label)\n",
    "\n",
    "            if phase == 'train':\n",
    "                StepLR.step()\n",
    "            \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            list = [epoch,phase,epoch_loss,format(epoch_acc)]\n",
    "            data = pd.DataFrame([list])\n",
    "            data.to_csv('train_model.csv',mode='a',header=False,index = False)\n",
    "            logger.info('Epoch:[{}/{}]\\t {}\\t loss={:.4f}\\t acc={:.4f}'.format(epoch, num_epochs,phase,epoch_loss,epoch_acc))\n",
    "\n",
    "        print('Done!')\n",
    "        time_elapsed=time.time()-since\n",
    "        logger.info('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60,time_elapsed % 60))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8b468-1d72-40ac-90a5-57e93e11cb40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model=train_model(model,loss_fn,optimizer,StepLR,num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44217f79-e5b7-4a29-8463-ee1630182b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'train_model.pth')\n",
    "torch.save(model.state_dict(), 'train_model_weight.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLS",
   "language": "python",
   "name": "tls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
